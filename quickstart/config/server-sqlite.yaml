# CORINT Decision Engine Server Configuration
# Note: If running in a real environment, please rename this file to server.yaml and modify the configuration items according to your actual production environment.

# Server configuration
server:
  # Server host (127.0.0.1 for localhost only, 0.0.0.0 for all interfaces)
  host: "127.0.0.1"

  # Server port
  port: 8080

  # gRPC Server port (optional, if not set, gRPC server will not start)
  grpc_port: 50051

  # Enable metrics collection
  enable_metrics: true

  # Enable distributed tracing
  enable_tracing: true

  # Log level (trace, debug, info, warn, error)
  log_level: "info"

# Repository configuration for loading rules and pipelines
# Supports three types: filesystem, database, api
repository:
  type: filesystem
  # Base path for repository (contains registry.yaml and pipelines/)
  path: "repository"

# Alternative repository configurations (examples):

# Database repository using datasource reference
# The datasource should be defined in the 'datasources' section below
# repository:
#   type: database
#   datasource: postgres_rules  # References datasource defined in 'datasources' section

# Legacy database repository (still supported for backward compatibility)
# repository:
#   type: database
#   db_type: postgresql
#   url: "postgresql://user:password@localhost:5432/corint_rules"

# API repository
# repository:
#   type: api
#   base_url: "https://api.example.com/rules"
#   api_key: "your-api-key-here"  # optional

# Data Sources Configuration
#
# All datasources are defined here, including:
# - Repository storage (rules, pipelines, etc.)
# - Feature calculation (events, aggregations, lookups)
# - User authentication/authorization
# - System-level data storage
#
# These datasources are used by both the server (for repository storage) and
# the SDK (for feature computation). No need to define them separately.
datasource:
  # ============================================================================
  # Repository Storage Datasources
  # ============================================================================
  
 

  # SQLite datasource for repository storage (testing/small deployments)
  sqlite_rules:
    type: sql
    provider: sqlite
    connection_string: "sqlite://./data/corint_rules.db"
    database: "corint_rules"
    options:
      max_connections: "3"

  # PostgreSQL datasource for user authentication/authorization
  # postgres_auth:
  #   type: sql
  #   provider: postgresql
  #   connection_string: "postgresql://user:password@localhost:5432/corint_auth"
  #   database: "corint_auth"
  #   options:
  #     max_connections: "10"

  # ============================================================================
  # Feature Calculation Datasources (for event aggregation and lookups)
  # ============================================================================

  # SQLite datasource for event storage (development/testing)
  events_datasource:
    type: sql
    provider: sqlite
    # Database file in project root directory
    # Note: sqlite:// URI requires absolute path or path relative to current working directory
    connection_string: "sqlite://corint.db"
    database: "corint"
    events_table: "events"
    options:
      max_connections: "5"

   

  # Redis datasource for feature lookups and caching
  lookup_datasource:
    type: feature_store
    provider: redis
    connection_string: "redis://localhost:6379/0"
    options:
      namespace: "user_features"
      default_ttl: "3600"
      max_connections: "50"
      min_idle: "5"
      connection_timeout: "30"

 

# LLM Provider Configuration
# Supports OpenAI (including O1/O3 thinking models), Anthropic (Claude with extended thinking),
# Google Gemini, and DeepSeek
llm:
  # Default provider to use (openai, anthropic, gemini, deepseek)
  default_provider: deepseek

  # Enable response caching to reduce API calls
  enable_cache: true

  # Default settings for thinking/reasoning models
  # When enabled, compatible models will use extended reasoning
  enable_thinking: false

  # OpenAI Configuration
  openai:
    # API key (can also use OPENAI_API_KEY environment variable)
    # Supports standard models (GPT-4, GPT-3.5) and thinking models (O1/O3)
    api_key: "${OPENAI_API_KEY}"

    # Base URL (optional, for Azure OpenAI or custom endpoints)
    base_url: "https://api.openai.com/v1"

    # Default model for standard text generation
    default_model: "gpt-4o-mini"

    # Default max tokens for responses
    max_tokens: 1000

    # Default temperature (0.0 - 1.0, not used for O1/O3 models)
    temperature: 0.7

  # Anthropic Configuration
  # Supports Claude models with extended thinking mode
  anthropic:
    # API key (can also use ANTHROPIC_API_KEY environment variable)
    api_key: "${ANTHROPIC_API_KEY}"

    # Default Claude model
    default_model: "claude-3-5-sonnet-20241022"

    # Extended thinking budget in tokens (used when enable_thinking is true)
    thinking_budget: 10000

    # Default max tokens for responses
    max_tokens: 1000

    # Default temperature (0.0 - 1.0)
    temperature: 0.7

  # Google Gemini Configuration
  # Standard text generation models
  gemini:
    # API key (can also use GEMINI_API_KEY environment variable)
    api_key: "${GEMINI_API_KEY}"

    # Default Gemini model
    default_model: "gemini-1.5-flash"

    # Default max tokens for responses
    max_tokens: 1000

    # Default temperature (0.0 - 1.0)
    temperature: 0.7

  # DeepSeek Configuration
  # OpenAI-compatible API
  deepseek:
    # API key (can also use DEEPSEEK_API_KEY environment variable)
    api_key: "${DEEPSEEK_API_KEY}"

    # Default DeepSeek model
    default_model: "deepseek-chat"

    # Default max tokens for responses
    max_tokens: 1000

    # Default temperature (0.0 - 1.0)
    temperature: 0.7

# external api configuration
api:
  ipinfo:
    token: "a63066c9a63590"