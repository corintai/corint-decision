# CORINT Decision Engine Server Configuration

# Server host (127.0.0.1 for localhost only, 0.0.0.0 for all interfaces)
host: "127.0.0.1"

# Server port
port: 8080

# Repository configuration for loading rules and pipelines
# Supports three types: filesystem, database, api
repository:
  type: filesystem
  # Base path for repository (contains registry.yaml and pipelines/)
  path: "repository"

# Alternative repository configurations (examples):

# Database repository using datasource reference
# The datasource should be defined in the 'datasources' section below
# repository:
#   type: database
#   datasource: postgres_rules  # References datasource defined in 'datasources' section

# Legacy database repository (still supported for backward compatibility)
# repository:
#   type: database
#   db_type: postgresql
#   url: "postgresql://user:password@localhost:5432/corint_rules"

# API repository
# repository:
#   type: api
#   base_url: "https://api.example.com/rules"
#   api_key: "your-api-key-here"  # optional

# Data Sources Configuration
#
# Server-level datasources for:
# - Repository storage (rules, pipelines, etc.)
# - User authentication/authorization
# - System-level data storage
#
# Note: Feature calculation datasources are defined in repository/configs/datasources/
# and are automatically loaded by the SDK for feature computation.
#
# Configuration Strategy:
# - Small deployments: Can use same database for both (rules + events), but must configure twice
#   Example: Define 'postgres_main' here and also in repository/configs/datasources/postgres_events.yaml
# - Large deployments: Separate databases (rules in one DB, events in another for scalability)
#   Example: 'postgres_rules' here, 'postgres_events' in repository/configs/datasources/
#
# Usage in repository configuration:
#   repository:
#     type: database
#     datasource: postgres_rules  # References a datasource defined below
datasources:
  # PostgreSQL datasource for repository storage (rules, pipelines, etc.)
  postgres_rules:
    type: sql
    provider: postgresql
    connection_string: "postgresql://user:password@localhost:5432/corint_rules"
    database: "corint_rules"
    options:
      max_connections: "5"

  # SQLite datasource for repository storage (testing/small deployments)
  sqlite_rules:
    type: sql
    provider: sqlite
    connection_string: "sqlite://./data/corint_rules.db"
    database: "corint_rules"
    options:
      max_connections: "3"

  # PostgreSQL datasource for user authentication/authorization
  # postgres_auth:
  #   type: sql
  #   provider: postgresql
  #   connection_string: "postgresql://user:password@localhost:5432/corint_auth"
  #   database: "corint_auth"
  #   options:
  #     max_connections: "10"

# Enable metrics collection
enable_metrics: true

# Enable distributed tracing
enable_tracing: true

# Log level (trace, debug, info, warn, error)
log_level: "info"

# LLM Provider Configuration
# Supports OpenAI (including O1/O3 thinking models), Anthropic (Claude with extended thinking),
# Google Gemini, and DeepSeek
llm:
  # Default provider to use (openai, anthropic, gemini, deepseek)
  default_provider: deepseek

  # Enable response caching to reduce API calls
  enable_cache: true

  # Default settings for thinking/reasoning models
  # When enabled, compatible models will use extended reasoning
  enable_thinking: false

  # OpenAI Configuration
  openai:
    # API key (can also use OPENAI_API_KEY environment variable)
    # Supports standard models (GPT-4, GPT-3.5) and thinking models (O1/O3)
    api_key: "${OPENAI_API_KEY}"

    # Base URL (optional, for Azure OpenAI or custom endpoints)
    base_url: "https://api.openai.com/v1"

    # Default model for standard text generation
    default_model: "gpt-4o-mini"

    # Default max tokens for responses
    max_tokens: 1000

    # Default temperature (0.0 - 1.0, not used for O1/O3 models)
    temperature: 0.7

  # Anthropic Configuration
  # Supports Claude models with extended thinking mode
  anthropic:
    # API key (can also use ANTHROPIC_API_KEY environment variable)
    api_key: "${ANTHROPIC_API_KEY}"

    # Default Claude model
    default_model: "claude-3-5-sonnet-20241022"

    # Extended thinking budget in tokens (used when enable_thinking is true)
    thinking_budget: 10000

    # Default max tokens for responses
    max_tokens: 1000

    # Default temperature (0.0 - 1.0)
    temperature: 0.7

  # Google Gemini Configuration
  # Standard text generation models
  gemini:
    # API key (can also use GEMINI_API_KEY environment variable)
    api_key: "${GEMINI_API_KEY}"

    # Default Gemini model
    default_model: "gemini-1.5-flash"

    # Default max tokens for responses
    max_tokens: 1000

    # Default temperature (0.0 - 1.0)
    temperature: 0.7

  # DeepSeek Configuration
  # OpenAI-compatible API
  deepseek:
    # API key (can also use DEEPSEEK_API_KEY environment variable)
    api_key: "${DEEPSEEK_API_KEY}"

    # Default DeepSeek model
    default_model: "deepseek-chat"

    # Default max tokens for responses
    max_tokens: 1000

    # Default temperature (0.0 - 1.0)
    temperature: 0.7